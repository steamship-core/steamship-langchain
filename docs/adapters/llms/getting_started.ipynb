{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "939e05c5",
   "metadata": {},
   "source": [
    "# Getting Started \n",
    "\n",
    "This notebook demonstrates how to use Steamship compatible LLMs in LangChain app. \n",
    "Steamship compatible LLMs respect the LangChain's standard LLM interface and can easily be swapped by updating the import statement.\n",
    "Doing so will give you access to our distributed that scale with the number of requests. \n",
    "Since these LLMs are run in Steamship's cloud infrastructure you won't have to worry about installing upstream dependencies to run these LLMs. \n",
    "\n",
    "All our LLMs come preloaded with Api Keys. For more information about API Key management and billing, please read our access & billing section.\n",
    "\n",
    "More information about LangChain's LLM interface can be found [here](https://langchain.readthedocs.io/en/latest/modules/llms/getting_started.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8502a0d",
   "metadata": {},
   "source": [
    "## Supported LLM Providers\n",
    "\n",
    "Steamship supports 1 LLM provider. Support for more LLM providers is on its way. \n",
    "\n",
    "\n",
    "| Provider    | LangChain            | Steamship                       | \n",
    "|-------------|----------------------|---------------------------------|\n",
    "| OpenAI      | langchain.llms.OpenAI | steamship_langchain.llms.OpenAI |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550242a9",
   "metadata": {},
   "source": [
    "To use our adapter, import one of the supported LLMs from `steamship_langchain.llms` and connect it to an authenticated steamship client as follows: \n",
    "\n",
    "```diff\n",
    "- from langchain.llms import OpenAI\n",
    "+ from steamship_langchain.llms import OpenAI\n",
    "+ from steamship import Steamship\n",
    "\n",
    "+ client = Steamship()\n",
    "\n",
    "llm = OpenAI(\n",
    "+ client=client,\n",
    "  model_name=\"text-ada-001\", \n",
    "  n=2, \n",
    "  best_of=2,\n",
    "  temperature=0.9)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82ae9f3",
   "metadata": {},
   "source": [
    "## Example \n",
    "\n",
    "For this example, we will work with an OpenAI LLM wrapper, although the functionalities highlighted are generic for all LLM types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c108786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion: \n",
      "\n",
      "A man walks into a bar and asks for a beer. The bartender says \"You're out of luck. We've been closed for fifteen minutes.\"\n"
     ]
    }
   ],
   "source": [
    "from steamship_langchain.llms import OpenAI\n",
    "from steamship import Steamship\n",
    "\n",
    "client = Steamship()\n",
    "\n",
    "llm = OpenAI(client=client, model_name=\"text-ada-001\", n=2, best_of=2, temperature=0.9)\n",
    "\n",
    "print(\"Completion:\", llm(\"Tell me a joke\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b462ad59",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_result = llm.generate([\"Tell me a joke\", \"Tell me a poem\"] * 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00530113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_completions 30\n"
     ]
    }
   ],
   "source": [
    "print(\"n_completions\", len(llm_result.generations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0246ae9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first completion [Generation(text='\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side.', generation_info=None), Generation(text='\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side.', generation_info=None)]\n"
     ]
    }
   ],
   "source": [
    "print(\"first completion\", llm_result.generations[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c9d065c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last completion [Generation(text=\"\\n\\nA rose by the side of the road\\n\\nIs all I need to find my way\\n\\nTo the place I've been searching for\\n\\nAnd my heart is singing with joy\\n\\nWhen I look at this rose\\n\\nIt reminds me of the love I've found\\n\\nAnd I know that wherever I go\\n\\nI'll always find my rose by the side of the road.\", generation_info=None), Generation(text=\"\\n\\nThe world is a beautiful place,\\nThe colors are so bright and true,\\nAnd I feel so free and free,\\nWhen I'm away from here.\\n\\nThe sky is so blue,\\nAnd the sun is so warm,\\nAnd I feel so free and free,\\nWhen I'm away from here.\", generation_info=None)]\n"
     ]
    }
   ],
   "source": [
    "print(\"last completion\", llm_result.generations[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1424242a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llm_output {'token_usage': {'prompt_tokens': 120, 'completion_tokens': 4200, 'total_tokens': 4320}}\n"
     ]
    }
   ],
   "source": [
    "print(\"llm_output\", llm_result.llm_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2a3620c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of tokens 3\n"
     ]
    }
   ],
   "source": [
    "print(\"number of tokens\", llm.get_num_tokens(\"what a joke\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15 (default, Oct 11 2022, 21:31:25) \n[Clang 14.0.0 (clang-1400.0.29.102)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "834117f4e0f61aee809513abffa40d5ecdde34fc37831cd637d73e61f5d7de69"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
