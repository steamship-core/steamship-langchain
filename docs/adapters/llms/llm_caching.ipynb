{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1eb06779",
   "metadata": {},
   "source": [
    "# LLM Caching \n",
    "\n",
    "This example demonstrates how to cache results of individual LLM calls using Steamship's Cache. \n",
    "Steamship's Cache follow LangChain's LLM Caching interface and can easily be swapped by updating the import statement.\n",
    "Doing so will give you access to a cloud-hosted cache linked to your workspace. \n",
    "Using a single cache for your workspace means you can share your cache across different LangChain pipelines. \n",
    "Including pipelines from other users that are deployed in your workspace. \n",
    "Since these LLMs are run in Steamship's cloud infrastructure you won't have to worry about installing upstream dependencies or scaling issues linked to local operation. \n",
    "\n",
    "Today, we don't charge for LLM Caching. For more information about billing, please read our access & billing section.\n",
    "\n",
    "More information about LangChain's LLM Caching can be found [here](https://langchain.readthedocs.io/en/latest/modules/llms/examples/llm_caching.html)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d7ede01e",
   "metadata": {},
   "source": [
    "## Steamship Cache\n",
    "\n",
    "Steamship offers one Cache implementation called `SteamshipCache` which serves as a one-size-fits-all replacement for the all caches offered by LangChain.\n",
    "\n",
    "\n",
    "| Type       | LangChain            | Steamship Replacement                    | \n",
    "|------------|----------------------|------------------------------------------|\n",
    "| In Memory  | langchain.cache.InMemoryCache | steamship_langchain.cache.SteamshipCache |\n",
    "| SQLite     | langchain.cache.SQLiteCache | steamship_langchain.cache.SteamshipCache         |\n",
    "| Redis      | langchain.cache.RedisCache | steamship_langchain.cache.SteamshipCache         |\n",
    "| SQLAlchemy | langchain.cache.SQLAlchemyCache | steamship_langchain.cache.SteamshipCache          |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88bdbb1",
   "metadata": {},
   "source": [
    "To use our adapter, simply replace your Cache with `steamship_langchain.cache.SteamshipCache` and connect it to an authenticated steamship client as follows: "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0a45a234",
   "metadata": {},
   "source": [
    "```diff\n",
    "import langchain\n",
    "+ from steamship import Steamship\n",
    "\n",
    "- from langchain.llms import OpenAI\n",
    "+ from steamship_langchain.llms import OpenAI\n",
    "- from langchain.cache import InMemoryCache\n",
    "+ from steamship_langchain.cache import SteamshipCache \n",
    "\n",
    "+ client = Steamship()\n",
    "- langchain.llm_cache = InMemoryCache()\n",
    "+ langchain.llm_cache = SteamshipCache(client=client)\n",
    "\n",
    "\n",
    "llm = OpenAI(\n",
    "+   client=client,\n",
    "    model_name=\"text-ada-001\", \n",
    "    n=2,\n",
    "    best_of=2, \n",
    "    temperature=0.9)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d39891",
   "metadata": {},
   "source": [
    "## Example \n",
    "\n",
    "For this example, we will work with an In Memory Cache, although the SteamshipCache is a drop-in replacement for all LLM Caches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46c7e6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "from steamship import Steamship\n",
    "\n",
    "from steamship_langchain.llms import OpenAI\n",
    "from steamship_langchain.cache import SteamshipCache\n",
    "\n",
    "client = Steamship()\n",
    "langchain.llm_cache = SteamshipCache(client=client)\n",
    "\n",
    "\n",
    "llm = OpenAI(client=client, model_name=\"text-ada-001\", n=2, best_of=2, temperature=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93513261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 49.3 ms, sys: 9.4 ms, total: 58.7 ms\n",
      "Wall time: 7.13 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "llm(\"Tell me a joke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "281f82d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.56 ms, sys: 1.44 ms, total: 5 ms\n",
      "Wall time: 106 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "llm(\"Tell me a joke\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "834117f4e0f61aee809513abffa40d5ecdde34fc37831cd637d73e61f5d7de69"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
